{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert tokenized codesamples to word2vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/scheuererra68323/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FuncDef_TokenAnon = pd.read_parquet(r\"/mnt/md0/user/scheuererra68323/JTT/JTT_TokenAnon_wExtFuncCalls_Labeled.parquet\")\n",
    "#FuncDef_TokenAnon = pd.read_parquet(r\"/mnt/md0/user/scheuererra68323/LO_SARD102/LO_SARD102_TokenAnon_wExtFuncCalls_Labeled.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = FuncDef_TokenAnon.copy()\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Word2Vec implementation and t-SNE visualization derived from \n",
    "# https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "#\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __init__(self, _sentences):\n",
    "        self._sentences = _sentences\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for _sentence in self._sentences:\n",
    "            yield _sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "\n",
    "sentences = MyCorpus(df['token_anon'])\n",
    "model = gensim.models.Word2Vec(sentences=sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 9697 tokens\n",
      "unsigned\n",
      "int\n",
      "identifier0\n",
      ";\n",
      "=\n",
      "<numeric_constant>\n",
      "if\n",
      "(\n",
      "globalReturnsTrue\n",
      ")\n",
      "{\n",
      "identifier2\n",
      "identifier3\n",
      ",\n",
      "<string_literal>\n",
      "&\n",
      "}\n",
      "identifier4\n",
      "+\n",
      "printUnsignedLine\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary contains {} tokens\".format(len(model.wv.vocab)))\n",
    "for i, word in enumerate(model.wv.vocab):\n",
    "    if i == 20:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    vectors = [] # positions in vector space\n",
    "    labels = [] # keep track of words to label our data again later\n",
    "    for word in model.wv.vocab:\n",
    "        vectors.append(model.wv[word])\n",
    "        labels.append(word)\n",
    "\n",
    "    # convert both lists into numpy vectors for reduction\n",
    "    vectors = np.asarray(vectors)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    vectors = np.asarray(vectors)\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels, num_annotate, filename=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(100,100))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    #\n",
    "    # Label randomly subsampled data points\n",
    "    #\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, num_annotate)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "    \n",
    "    if filename is not None:\n",
    "        plt.savefig(filename)\n",
    "    plt.show()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "#plot_with_matplotlib(x_vals, y_vals, labels, num_annotate=len(labels), filename=\"JTT_word2vec.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert tokenized into vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_word2vec_vectors(token_anon):\n",
    "    _vectors = []\n",
    "    for tkn in token_anon:\n",
    "        _vectors.append(model.wv.get_vector(tkn))\n",
    "    return _vectors\n",
    "\n",
    "FuncDef_TokenAnon['word2vec'] = FuncDef_TokenAnon.token_anon.apply(get_word2vec_vectors)\n",
    "\n",
    "print(FuncDef_TokenAnon.shape)\n",
    "#FuncDef_TokenAnon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Create a single vector for each sample (avg over all word vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 292724 entries, 0 to 292723\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   word2vec_avg   292724 non-null  object \n",
      " 1   is_vulnerable  292724 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 4.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                        word2vec_avg  is_vulnerable\n",
       " 0  [-1.0249654, 0.375369, 0.55047935, 0.7751315, ...            0.0\n",
       " 1  [-1.0249654, 0.375369, 0.55047935, 0.7751315, ...            0.0\n",
       " 2  [-0.5463387, 0.17863393, 0.30269822, 0.9715602...            0.0\n",
       " 3  [-0.5565288, -0.19019793, 0.38513714, 0.728798...            0.0\n",
       " 4  [-0.44383517, 1.0338036, -0.22511859, 0.872823...            0.0,\n",
       " None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = FuncDef_TokenAnon.copy()\n",
    "\n",
    "dataset['word2vec_avg'] = dataset.word2vec.apply( lambda x : np.mean(x, axis=0) )\n",
    "\n",
    "dataset = dataset.reset_index()[['word2vec_avg', 'is_vulnerable']]\n",
    "dataset.head(), dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py:2431: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['word2vec_avg'], dtype='object')]\n",
      "\n",
      "  pytables.to_hdf(\n"
     ]
    }
   ],
   "source": [
    "dataset.to_hdf(\"/mnt/md0/user/scheuererra68323/JTT_word2vec.h5\", key=\"JTT_word2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Save zero-padded word2vec sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== word2vec sequences ===\n",
      "max sequence length (original): 11398\n",
      "using examples up to sequence length = 100\n",
      "using 21851 of 24999 examples. (87.41%)\n",
      "shape: (21851, 125)\n"
     ]
    }
   ],
   "source": [
    "dataset_seq = FuncDef_TokenAnon.copy().drop(['code_snippet', 'dump_tokens_output', 'token_anon'], axis=1)\n",
    "print(\"=== word2vec sequences ===\")\n",
    "print(\"max sequence length (original):\", dataset_seq.word2vec.map(len).max())\n",
    "\n",
    "print(\"using examples up to sequence length = 100\")\n",
    "max_sequence_length = 100\n",
    "dataset_seq = dataset_seq.loc[dataset_seq.word2vec.map(len) <= max_sequence_length]\n",
    "print(f\"using {dataset_seq.shape[0]} of {FuncDef_TokenAnon.shape[0]} examples. ({dataset_seq.shape[0] / FuncDef_TokenAnon.shape[0] * 100:.2f}%)\")\n",
    "print(\"shape:\", dataset_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st example before padding:  (49, 100)\n",
      "1st example after padding:  (100, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(21851, 121)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"1st example before padding: \", np.array(dataset_seq.word2vec.iloc[0]).shape)\n",
    "\n",
    "dataset_seq['word2vec_seq'] = dataset_seq.word2vec.map(lambda _sequence:  \n",
    "    # axis 0: zero-padding of 580-length to the right\n",
    "    # axis 1: no padding\n",
    "    np.pad(np.array(_sequence),\n",
    "    pad_width= ( [[0, max_sequence_length - np.array(_sequence).shape[0]], [0, 0]]),\n",
    "    mode='constant',\n",
    "    constant_values=0)\n",
    ")\n",
    "print(\"1st example after padding: \", np.array(dataset_seq.word2vec_seq.iloc[0]).shape)\n",
    "\n",
    "dataset_seq = dataset_seq.drop(['path', 'line_start', 'line_stop', 'external_function_names', 'word2vec'], axis=1)\n",
    "dataset_seq.shape\n",
    "#dataset_seq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21851, 100, 100)\n",
      "(21851, 120)\n"
     ]
    }
   ],
   "source": [
    "X_sequence = np.stack(dataset_seq.word2vec_seq)\n",
    "y = dataset_seq.drop(['word2vec_seq'], axis=1)\n",
    "print(X_sequence.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "savename = 'JTT'\n",
    "\n",
    "np.savez_compressed(\n",
    "    f'/mnt/md0/user/scheuererra68323/{savename}_word2vec_X_sequence.npz',\n",
    "    X_sequence\n",
    ")\n",
    "y.to_hdf(f'/mnt/md0/user/scheuererra68323/{savename}_word2vec_y_sequence.h5', key=f'{savename}_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
