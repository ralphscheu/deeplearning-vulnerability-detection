{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "from collections import Counter\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "###\n",
    "# IMBALANCED CLASSES MITIGATION\n",
    "###\n",
    "\n",
    "def undersample(X, y):\n",
    "    ''' Undersamples good code samples so we get equally large sets for both classes'''\n",
    "    \n",
    "    dataset = pd.DataFrame({\"X\": list(X), \"y\": y})\n",
    "    \n",
    "    good = dataset[dataset.y == 0].sample(n=dataset.y.value_counts()[1], random_state=0)\n",
    "    bad  = dataset[dataset.y == 1]\n",
    "    dataset_balanced = pd.concat([good,bad], axis=0)\n",
    "    \n",
    "    return numpy.vstack(dataset_balanced.X), dataset_balanced.y\n",
    "    \n",
    "    \n",
    "def oversample_smote(X, y):\n",
    "    print (\"imbalanced_learn version \", imblearn.__version__)\n",
    "\n",
    "    # summarize the new class distribution\n",
    "    counter = Counter(y)\n",
    "    print(\"Counter output before SMOTE:\", counter)\n",
    "    \n",
    "    # transform the dataset\n",
    "    oversample = SMOTE()\n",
    "    X, y = oversample.fit_resample(X, y)\n",
    "    \n",
    "    # summarize the new class distribution\n",
    "    counter = Counter(y)\n",
    "    print(\"Counter output after SMOTE:\", counter)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def no_mitigation(X_train, Y_train):\n",
    "    return X_train, Y_train\n",
    "\n",
    "\n",
    "##\n",
    "# MODELS\n",
    "##\n",
    "\n",
    "def create_model_MLP(init_mode='glorot_uniform', num_classes=2):\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(64,\n",
    "                         input_dim=100,\n",
    "                         kernel_initializer=init_mode,\n",
    "                         activation=tf.nn.relu),\n",
    "            \n",
    "            \n",
    "            layers.Dropout(0.1),\n",
    "                        \n",
    "            layers.Dense(64,\n",
    "                         kernel_initializer=init_mode,\n",
    "                         activation=tf.nn.relu),\n",
    "            \n",
    "            layers.Dense(32,\n",
    "                         kernel_initializer=init_mode,\n",
    "                         activation=tf.nn.relu),\n",
    "            \n",
    "            \n",
    "            layers.Dense(num_classes, \n",
    "                         kernel_initializer=init_mode, \n",
    "                         activation=tf.nn.softmax),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def create_MLP(init_mode='glorot_uniform', hidden_size=10, num_classes=2):\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(hidden_size,\n",
    "                         input_dim=100,\n",
    "                         kernel_initializer=init_mode,\n",
    "                         activation=tf.nn.relu),            \n",
    "            \n",
    "            layers.Dropout(0.1),\n",
    "                        \n",
    "            layers.Dense(hidden_size,\n",
    "                         kernel_initializer=init_mode,\n",
    "                         activation=tf.nn.relu),         \n",
    "            \n",
    "            layers.Dense(hidden_size,\n",
    "                         kernel_initializer=init_mode,\n",
    "                         activation=tf.nn.relu),             \n",
    "            \n",
    "            layers.Dense(num_classes, \n",
    "                         kernel_initializer=init_mode, \n",
    "                         activation=tf.nn.softmax),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def create_model_LSTM():\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            layers.LSTM(2, input_shape=(24, 768)),\n",
    "            layers.Dropout(0.1),\n",
    "            layers.Activation('softmax')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "###\n",
    "# TRAINING EVALUATION\n",
    "###\n",
    "\n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label=\"train loss\")\n",
    "    plt.plot(history.history['val_loss'], label=\"val loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_accuracy(history):\n",
    "    plt.plot(history.history['accuracy'], label=\"train accuracy\")\n",
    "    plt.plot(history.history['val_accuracy'], label=\"val accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_confusion_matrix(model, X_test, Y_test):\n",
    "    Y_pred = model.predict(X_test)\n",
    "    return confusion_matrix(numpy.argmax(Y_test, axis=1), numpy.argmax(Y_pred, axis=1))\n",
    "    \n",
    "\n",
    "def print_metrics(conf_matrix, Y_test):    \n",
    "    # confusion matrix components\n",
    "    # [[TN, FP]\n",
    "    #  [FN, TP]]\n",
    "    tn = conf_matrix[0,0]\n",
    "    fn = conf_matrix[1,0]\n",
    "    fp = conf_matrix[0,1]\n",
    "    tp = conf_matrix[1,1]\n",
    "    \n",
    "    # confusion matrix components as percentages\n",
    "    tp_percent = tp / len(Y_test) * 100\n",
    "    tn_percent = tn / len(Y_test) * 100\n",
    "    fp_percent = fp / len(Y_test) * 100\n",
    "    fn_percent = fn / len(Y_test) * 100\n",
    "    \n",
    "    \n",
    "    print(\"Confusion matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print()\n",
    "    print(\"Confusion matrix (Percentages):\")\n",
    "    print(numpy.array([\n",
    "        [round(tn_percent, 3), round(fp_percent, 3)],\n",
    "        [round(fn_percent, 3), round(tp_percent, 3)]\n",
    "    ]))\n",
    "    \n",
    "    print(\"\\nMetrics:\")\n",
    "    tpr = tp / (tp + fn)\n",
    "    tnr = tn / (tn + fp)\n",
    "    \n",
    "    fpr = fp / (fp + tn)\n",
    "    fnr = fn / (fn + tp)\n",
    "    \n",
    "    print(\"Sensitivity (TPR): {:8.6f}\".format( tpr ))\n",
    "    print(\"Specifity (TNR):   {:8.6f}\".format( tnr ))\n",
    "    print()\n",
    "    print(\"FPR: {:8.6f}\".format( fpr ))\n",
    "    print(\"FNR: {:8.6f}\".format( fnr ))\n",
    "    print()\n",
    "    print(\"Balanced accuracy: {:8.6f}\".format( (tpr + tnr) /2 ))\n",
    "    \n",
    "    \n",
    "    \n",
    "##############################\n",
    "\n",
    "\n",
    "# https://towardsdatascience.com/keras-data-generators-and-how-to-use-them-b69129ed779c\n",
    "\n",
    "from tensorflow.keras.utils import Sequence\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, _X, _y, to_fit=True, \n",
    "                 batch_size=256, n_classes=2):\n",
    "        \n",
    "        \"\"\"Initialization\n",
    "        :param list_IDs: list of all 'label' ids to use in the generator\n",
    "        :param labels: list of image labels (file names)\n",
    "        :param to_fit: True to return X and y, False to return X only\n",
    "        :param batch_size: batch size at each iteration\n",
    "        :param dim: tuple indicating image dimension\n",
    "        :param n_classes: number of output masks\n",
    "        \"\"\"\n",
    "        self._X = _X\n",
    "        self._y = _y\n",
    "        self.to_fit = to_fit\n",
    "        self.batch_size = batch_size\n",
    "        self.n_classes = n_classes\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return int(numpy.ceil(self._X.shape[0] / self.batch_size))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\n",
    "        :param index: index of the batch\n",
    "        :return: X and y when fitting. X only when predicting\n",
    "        \"\"\"\n",
    "        #print(f\"Returning samples [{index * self.batch_size}:{(index + 1) * self.batch_size}]\")\n",
    "        if self.to_fit:\n",
    "            return self._X[index * self.batch_size:(index + 1) * self.batch_size, :, :], self._y[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        else:\n",
    "            return self._X[index * self.batch_size:(index + 1) * self.batch_size, :, :]\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
